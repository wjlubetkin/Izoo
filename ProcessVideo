"""
Video Face Detection + Recognition Pipeline for Chimp Exhibit
Processes video files with frame sampling, face detection, and running prediction averages
Optimized for memory efficiency with 16GB VRAM / 64GB RAM constraints
"""

from __future__ import annotations

import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Deque
from collections import deque, defaultdict
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from dataclasses import dataclass
import gc
import time
import warnings
warnings.filterwarnings('ignore')

# ----------------------------- CONFIGURATION ---------------------------------
# Face Detection Settings
MIN_DETECTION_CONFIDENCE = 0.3
MODEL_SELECTION = 0  # 0 for short-range (2m), 1 for full-range (5m)

# ConvNeXt Model Settings  
IMG_SIZE = (224, 224)
MODEL_PATH = Path("convnext_finetuned_labelsmooth.keras")
CLASS_NAMES_TXT = Path("convnext_finetuned_labelsmooth.classes.txt")
TOP_K = 3

# Video Processing Settings
FRAME_SAMPLE_RATE = 10  # Process every 10th frame
RUNNING_AVERAGE_WINDOW = 30  # Keep last 30 predictions for averaging
BATCH_SIZE = 4  # Process faces in batches to manage memory
MAX_FACES_PER_FRAME = 10  # Limit faces per frame for memory management

# Input/Output
INPUT_VIDEO_DIR = Path("./input_video")
OUTPUT_DIR = Path("./video_output")
FRAME_OUTPUT_DIR = OUTPUT_DIR / "processed_frames"
CROPS_OUTPUT_DIR = OUTPUT_DIR / "face_crops"

# Create output directories
OUTPUT_DIR.mkdir(exist_ok=True)
FRAME_OUTPUT_DIR.mkdir(exist_ok=True)
CROPS_OUTPUT_DIR.mkdir(exist_ok=True)
# -----------------------------------------------------------------------------

@dataclass
class FaceDetection:
    """Data class for face detection results"""
    bbox: Tuple[int, int, int, int]  # (x, y, width, height)
    confidence: float
    frame_number: int
    face_id: int  # Unique ID for this face in the frame

@dataclass
class FacePrediction:
    """Data class for face prediction results"""
    face_id: int
    frame_number: int
    predictions: List[Tuple[str, float]]  # [(class_name, confidence), ...]
    bbox: Tuple[int, int, int, int]

class RunningPredictor:
    """Manages running averages of predictions across frames"""
    
    def __init__(self, window_size: int = 30):
        self.window_size = window_size
        self.prediction_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
        self.frame_count = 0
    
    def add_predictions(self, predictions: List[FacePrediction]):
        """Add new predictions and update running averages"""
        self.frame_count += 1
        
        # For each prediction, add to history
        for pred in predictions:
            if pred.predictions:
                # Add the top prediction to history
                top_class, top_conf = pred.predictions[0]
                self.prediction_history[top_class].append({
                    'frame': self.frame_count,
                    'confidence': top_conf,
                    'face_id': pred.face_id
                })
    
    def get_running_average(self, min_occurrences: int = 3) -> Optional[Tuple[str, float]]:
        """Get the most likely prediction based on running average"""
        if not self.prediction_history:
            return None
        
        # Calculate weighted averages for each class
        class_scores = {}
        for class_name, history in self.prediction_history.items():
            if len(history) >= min_occurrences:
                # Weight recent predictions more heavily
                total_weight = 0
                weighted_sum = 0
                
                for i, record in enumerate(history):
                    # More recent predictions get higher weight
                    weight = (i + 1) / len(history)
                    weighted_sum += record['confidence'] * weight
                    total_weight += weight
                
                if total_weight > 0:
                    class_scores[class_name] = weighted_sum / total_weight
        
        if not class_scores:
            return None
        
        # Return the class with highest weighted average
        best_class = max(class_scores.items(), key=lambda x: x[1])
        return best_class
    
    def get_recent_predictions_summary(self) -> Dict[str, int]:
        """Get count of predictions in recent window"""
        summary = {}
        for class_name, history in self.prediction_history.items():
            summary[class_name] = len(history)
        return summary

class VideoChimpPipeline:
    def __init__(self):
        # Initialize MediaPipe Face Detection
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=MODEL_SELECTION,
            min_detection_confidence=MIN_DETECTION_CONFIDENCE
        )
        
        # Load ConvNeXt model
        print("Loading ConvNeXt model...")
        self.convnext_model = tf.keras.models.load_model(MODEL_PATH, compile=False)
        self.num_classes = self.convnext_model.output_shape[-1]
        self.class_names = self._load_class_names()
        print(f"Model loaded with {self.num_classes} classes")
        
        # Initialize running predictor
        self.running_predictor = RunningPredictor(RUNNING_AVERAGE_WINDOW)
        
        # Memory management
        self.processed_frames = 0
        self.gc_frequency = 50  # Run garbage collection every 50 frames
        
    def _load_class_names(self) -> List[str]:
        """Load class names from file or generate defaults."""
        if CLASS_NAMES_TXT.exists():
            names = [line.strip() for line in CLASS_NAMES_TXT.read_text().splitlines() if line.strip()]
            if len(names) != self.num_classes:
                raise ValueError(f"Class count mismatch: {len(names)} vs {self.num_classes}")
            return names
        return [f"chimp_{i}" for i in range(self.num_classes)]
    
    def detect_faces(self, image: np.ndarray, frame_number: int) -> List[FaceDetection]:
        """Detect faces in image using MediaPipe."""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb_image)
        
        detections = []
        if results.detections:
            h, w = image.shape[:2]
            for i, detection in enumerate(results.detections[:MAX_FACES_PER_FRAME]):
                bbox = detection.location_data.relative_bounding_box
                
                # Convert relative coordinates to absolute pixels
                x = max(0, int(bbox.xmin * w))
                y = max(0, int(bbox.ymin * h))
                width = min(int(bbox.width * w), w - x)
                height = min(int(bbox.height * h), h - y)
                
                if width > 20 and height > 20:  # Filter out tiny detections
                    detections.append(FaceDetection(
                        bbox=(x, y, width, height),
                        confidence=detection.score[0],
                        frame_number=frame_number,
                        face_id=i
                    ))
        
        return detections
    
    def extract_face_crops(self, image: np.ndarray, detections: List[FaceDetection]) -> List[np.ndarray]:
        """Extract and resize face crops for model input."""
        crops = []
        for detection in detections:
            x, y, w, h = detection.bbox
            
            # Add padding around the face
            padding = 0.1
            pad_x = int(w * padding)
            pad_y = int(h * padding)
            
            # Expand bounding box with padding
            x1 = max(0, x - pad_x)
            y1 = max(0, y - pad_y)
            x2 = min(image.shape[1], x + w + pad_x)
            y2 = min(image.shape[0], y + h + pad_y)
            
            # Extract crop
            crop = image[y1:y2, x1:x2]
            
            # Resize to model input size
            crop_resized = cv2.resize(crop, IMG_SIZE)
            crops.append(crop_resized)
            
        return crops
    
    def recognize_faces_batch(self, face_crops: List[np.ndarray]) -> List[List[Tuple[str, float]]]:
        """Run ConvNeXt inference on face crops in batches."""
        if not face_crops:
            return []
        
        all_predictions = []
        
        # Process in batches to manage memory
        for i in range(0, len(face_crops), BATCH_SIZE):
            batch_crops = face_crops[i:i + BATCH_SIZE]
            
            # Convert to float32 and create batch
            input_batch = np.array(batch_crops, dtype=np.float32)
            
            # Run inference
            batch_predictions = self.convnext_model.predict(input_batch, verbose=0)
            
            # Get top-k predictions for each face in batch
            top_k = min(TOP_K, self.num_classes)
            rank = np.argsort(-batch_predictions, axis=1)[:, :top_k]
            
            for j in range(len(batch_crops)):
                preds = [(self.class_names[idx], float(batch_predictions[j, idx])) 
                        for idx in rank[j]]
                all_predictions.append(preds)
        
        return all_predictions
    
    def create_annotated_frame(self, image: np.ndarray, detections: List[FaceDetection], 
                              predictions: List[List[Tuple[str, float]]], 
                              frame_number: int, running_avg: Optional[Tuple[str, float]]) -> np.ndarray:
        """Create annotated frame with bounding boxes and predictions."""
        annotated = image.copy()
        
        # Draw bounding boxes and predictions
        for detection, preds in zip(detections, predictions):
            x, y, w, h = detection.bbox
            
            # Draw bounding box
            cv2.rectangle(annotated, (x, y), (x + w, y + h), (0, 0, 255), 2)
            
            # Add prediction label
            if preds:
                best_class, best_prob = preds[0]
                label = f"ID{detection.face_id}: {best_class} ({best_prob:.1%})"
                
                # Background rectangle for text
                (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                cv2.rectangle(annotated, (x, y - text_h - 10), (x + text_w, y), (255, 255, 0), -1)
                cv2.putText(annotated, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        # Add running average prediction at top of frame
        if running_avg:
            avg_class, avg_conf = running_avg
            avg_label = f"Running Avg: {avg_class} ({avg_conf:.1%})"
            cv2.rectangle(annotated, (10, 10), (400, 40), (0, 255, 0), -1)
            cv2.putText(annotated, avg_label, (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
        
        # Add frame number
        frame_label = f"Frame: {frame_number}"
        cv2.putText(annotated, frame_label, (10, annotated.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return annotated
    
    def process_video(self, video_path: Path, output_video_path: Optional[Path] = None) -> Dict:
        """Process entire video with face detection and recognition."""
        print(f"Processing video: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"Could not open video: {video_path}")
        
        # Get video properties
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames")
        print(f"Will process every {FRAME_SAMPLE_RATE} frames = {total_frames // FRAME_SAMPLE_RATE} frames")
        
        # Setup output video writer if requested
        output_writer = None
        if output_video_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_writer = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))
        
        # Processing statistics
        results = {
            'video_path': str(video_path),
            'total_frames': total_frames,
            'processed_frames': 0,
            'total_faces_detected': 0,
            'processing_time': 0,
            'final_prediction': None
        }
        
        frame_number = 0
        processed_count = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Process every Nth frame
                if frame_number % FRAME_SAMPLE_RATE == 0:
                    print(f"Processing frame {frame_number}/{total_frames}")
                    
                    # Detect faces
                    detections = self.detect_faces(frame, frame_number)
                    
                    if detections:
                        results['total_faces_detected'] += len(detections)
                        
                        # Extract face crops
                        face_crops = self.extract_face_crops(frame, detections)
                        
                        # Run recognition
                        predictions = self.recognize_faces_batch(face_crops)
                        
                        # Create FacePrediction objects
                        face_predictions = []
                        for detection, preds in zip(detections, predictions):
                            face_predictions.append(FacePrediction(
                                face_id=detection.face_id,
                                frame_number=frame_number,
                                predictions=preds,
                                bbox=detection.bbox
                            ))
                        
                        # Update running predictor
                        self.running_predictor.add_predictions(face_predictions)
                        
                        # Save individual face crops
                        for i, (detection, crop) in enumerate(zip(detections, face_crops)):
                            crop_filename = f"{video_path.stem}_frame_{frame_number:06d}_face_{i:02d}.jpg"
                            crop_path = CROPS_OUTPUT_DIR / crop_filename
                            cv2.imwrite(str(crop_path), crop)
                    
                    # Get running average prediction
                    running_avg = self.running_predictor.get_running_average()
                    
                    # Create annotated frame
                    annotated_frame = self.create_annotated_frame(
                        frame, detections, predictions if detections else [], 
                        frame_number, running_avg
                    )
                    
                    # Save annotated frame
                    frame_filename = f"{video_path.stem}_frame_{frame_number:06d}_annotated.jpg"
                    frame_path = FRAME_OUTPUT_DIR / frame_filename
                    cv2.imwrite(str(frame_path), annotated_frame)
                    
                    # Write to output video if requested
                    if output_writer:
                        output_writer.write(annotated_frame)
                    
                    processed_count += 1
                    
                    # Memory management
                    if processed_count % self.gc_frequency == 0:
                        gc.collect()
                        print(f"  Memory cleanup at frame {frame_number}")
                
                frame_number += 1
        
        finally:
            cap.release()
            if output_writer:
                output_writer.release()
        
        # Final results
        end_time = time.time()
        results['processed_frames'] = processed_count
        results['processing_time'] = end_time - start_time
        results['final_prediction'] = self.running_predictor.get_running_average()
        results['prediction_summary'] = self.running_predictor.get_recent_predictions_summary()
        
        print(f"\n=== PROCESSING COMPLETE ===")
        print(f"Processed {processed_count} frames in {results['processing_time']:.1f} seconds")
        print(f"Total faces detected: {results['total_faces_detected']}")
        print(f"Final prediction: {results['final_prediction']}")
        print(f"Prediction summary: {results['prediction_summary']}")
        
        return results
    
    def process_video_directory(self, input_dir: Path = INPUT_VIDEO_DIR) -> List[Dict]:
        """Process all videos in directory."""
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv'}
        video_files = [f for f in input_dir.rglob("*") 
                      if f.suffix.lower() in video_extensions]
        
        if not video_files:
            print(f"No video files found in {input_dir}")
            return []
        
        print(f"Found {len(video_files)} videos to process")
        
        all_results = []
        for video_path in sorted(video_files):
            try:
                # Create output video path
                output_video_path = OUTPUT_DIR / f"{video_path.stem}_processed.mp4"
                
                # Process video
                results = self.process_video(video_path, output_video_path)
                all_results.append(results)
                
                # Reset running predictor for next video
                self.running_predictor = RunningPredictor(RUNNING_AVERAGE_WINDOW)
                
            except Exception as e:
                print(f"Error processing {video_path.name}: {e}")
                continue
        
        return all_results

# Main execution
if __name__ == "__main__":
    # Initialize pipeline
    pipeline = VideoChimpPipeline()
    
    # Process all videos
    results = pipeline.process_video_directory()
    
    # Overall summary
    total_videos = len(results)
    total_processed_frames = sum(r['processed_frames'] for r in results)
    total_faces = sum(r['total_faces_detected'] for r in results)
    total_time = sum(r['processing_time'] for r in results)
    
    print(f"\n=== OVERALL SUMMARY ===")
    print(f"Videos processed: {total_videos}")
    print(f"Total frames processed: {total_processed_frames}")
    print(f"Total faces detected: {total_faces}")
    print(f"Total processing time: {total_time:.1f} seconds")
    print(f"Average FPS: {total_processed_frames / total_time:.1f}")
    
    print(f"\nOutput locations:")
    print(f"- Processed frames: {FRAME_OUTPUT_DIR}")
    print(f"- Face crops: {CROPS_OUTPUT_DIR}")
    print(f"- Processed videos: {OUTPUT_DIR}")
